{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID False Positive Rates versus Prevalence\n",
    "\n",
    "Early COVID-19 tests had very high false positive rates, which is often managable when tests are used along with other indicators of infection, but can produce spurious results when a very large number of tests are administered. In this notebook, we'll use the Specificity and Selectivity parameters from a recently published paper, [Development and Clinical Application of A Rapid IgM-IgG Combined Antibody Test for SARS-CoV-2 Infection Diagnosis](https://pubmed.ncbi.nlm.nih.gov/32104917/), which reports: \n",
    "\n",
    "> The overall testing sensitivity was 88.66% and specificity was 90.63%\n",
    "   \n",
    "Specificty and Selective are a bit hard to understand, but are well explained on their [Wikipedia page. ](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).  The important part to understand is the table in the [worked example](https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Worked_example). When a test is administered, there are four possible outcomes. The test can return a positive results, which can be a true positive or a false positive, or it can return a negative result, which is a true negative or a false negative. Of you organize those posibilities by what is the true condition ( does the patient have the vius or not ):\n",
    "\n",
    "* Patient has virus\n",
    " * True Positive ($\\mathit{TP}$)\n",
    " * False negative ($\\mathit{FN}$)\n",
    "* Patient does not have virus\n",
    " * True Negative ($\\mathit{TN}$)\n",
    " * False Positive.  ($\\mathit{FP}$)\n",
    "\n",
    "In the wikipedia table:\n",
    "\n",
    "* The number of people who do have the virus is  $\\mathit{TP}+\\mathit{FN}$, the true positives plus the false negatives, which are the cases that should have been reported positive, but were not. \n",
    "* The number or of people who do not have the virus is $\\mathit{TN}+\\mathit{FP}$, the true negatives and the false positives, which are the cases should have been reported positive, but were not. \n",
    "\n",
    "The values of Sensitivity and Specificity are defined as: \n",
    "\n",
    "$$\\begin{array}{ll}\n",
    "Sn = \\frac{\\mathit{TP}}{\\mathit{TP} + \\mathit{FN}} & \\text{True positives outcomes divided by all positive conditions} \\tag{1}\\label{eq1}\\\\ \n",
    "Sp = \\frac{\\mathit{TN}}{\\mathit{FP} + \\mathit{TN}} & \\text{True negatives outcomes divided by all negative conditions}\\\\ \n",
    "\\end{array}$$\n",
    "\n",
    "We want to know the number of false positives($\\mathit{FP}$) given the number of positive conditions ($\\mathit{TP}+\\mathit{FN}$) and the total number of tests. To compute these, we need to have some more information about the number of people tested, and how common the disease is: \n",
    "\n",
    "* Total test population $P$, the number of people being tested, which equals $\\mathit{TP}+\\mathit{FP}+\\mathit{FN}+\\mathit{TN}$\n",
    "* The prevalence $p$, the population rate of positive condition. \n",
    "\n",
    "We can do a little math to get: \n",
    "\n",
    "$$\\begin{array}{ll}\n",
    "\\mathit{TP} = Pp\\mathit{Sn} & \\text{}\\\\ \n",
    "\\mathit{FP} = P(1-p)(1-\\mathit{Sp}) \\text{}\\\\ \n",
    "\\mathit{TN} = P(1-p)\\mathit{Sp} & \\text{}\\\\ \n",
    "\\mathit{FN} = Pp(1-\\mathit{Sn})& \\text{}\\\\ \n",
    "\\end{array}$$\n",
    "\n",
    "You can see examples of these equations worked out in the third line in the red and green cells of the [Worked Example](https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Worked_example) on the Sensitivity and Specificity Wikipedia page. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the interesting questions when test results are reported is \"What percentage of the positive results are true positives?\" This is a particularly important question for the COVID-19 pandemic because there are a lot of reports that most people with the virus are asymptomatic. Are they really asymptomatic, or just false positives?\n",
    "\n",
    "The metric we're interested here is the portion of positive results that are true positives, the true positive rate, $\\mathit{TPR}$:\n",
    "\n",
    "$$\\mathit{TPR} = \\frac{\\mathit{TP} }{ \\mathit{TP} +\\mathit{FP}  } $$\n",
    "\n",
    "Which expands to:\n",
    "\n",
    "$$\\mathit{TPR} = \\frac{p\\mathit{Sn} }{ p\\mathit{Sn} + (1-p)(1-\\mathit{Sp})  } $$\n",
    "\n",
    "It is important to note that $\\mathit{TPR}$ is not dependent on $P$, the size of the population being tested. It depends only on the quality parameters of the test, $\\mathit{Sn}$ and $\\mathit{Sp}$, and the prevalence, $p$. For a given test, only the prevalence will change over time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aa52039c307f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_vs_tpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'p tpr'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "Sp = .9063\n",
    "Sn = .8866\n",
    "\n",
    "def p_vs_tpr(Sp, Sn):\n",
    "\n",
    "    for p in np.power(10,np.linspace(-7,np.log10(.5), num=100)): # range from 1 per 10m to 50%\n",
    "        tpr = (p*Sn) / ( (p*Sn)+(1-p)*(1-Sp))\n",
    "        yield (p, tpr)\n",
    "    \n",
    "df = pd.DataFrame(list(p_vs_tpr(Sp, Sn)), columns='p tpr'.split())\n",
    "df.head()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "df.plot(ax=ax, x='p',y='tpr', figsize=(10,10))\n",
    "\n",
    "fig.suptitle(f'Portion of Positives that Are True Vs Prevalence\\nFor test with Sp={Sp} and Sn={Sn}', fontsize=20)\n",
    "\n",
    "ax.set_xlabel('Condition Prevalence in Portion of Tested Population', fontsize=18)\n",
    "ax.set_ylabel('Portion of Positive Test Results that are True Positives', fontsize=18)\n",
    "\n",
    "\n",
    "#ax.set_xscale('log')\n",
    "#ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find the row with a TPR closest to 50%\n",
    "loss_min_idx = (df['tpr']-.5).abs().idxmin()\n",
    "df.iloc[loss_min_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gen_data():\n",
    "\n",
    "    for Sp in np.linspace(.9,1,num=20,endpoint=False):\n",
    "        for Sn in np.linspace(.88,1,num=20,endpoint=False):            \n",
    "            df = pd.DataFrame(list(p_vs_tpr(Sp, Sn)), columns='p tpr'.split())\n",
    "            # Find the row with a TPR closest to 50%\n",
    "            loss_min_idx = (df['tpr']-.5).abs().idxmin()\n",
    "            p = df.iloc[loss_min_idx].p\n",
    "            yield (Sp, Sn, p)\n",
    "\n",
    "df = pd.DataFrame(list(gen_data()), columns='Sp Sn p'.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = sns.load_dataset(\"flights\")\n",
    "\n",
    "flights = flights.pivot(\"month\", \"year\", \"passengers\")\n",
    "flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "\n",
    "t =df.pivot_table('p','Sp','Sn')\n",
    "\n",
    "ax = sns.heatmap(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate these values for the situation in the US when it first hit 100 cases, on March 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the date that the US first hit 100 cases?\n",
    "import metapack as mp\n",
    "pkg = mp.open_package('http://library.metatab.org/jhu.edu-covid19-1.zip')\n",
    "df = pkg.resource('confirmed').dataframe()\n",
    "df[df.location == 'US'].date_100.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that on March 3, using the test described in the article linked above, that the US Government was able to test the whole US population, and that the actual number of cases was 10X of the reported number. So we have: \n",
    "\n",
    "* $\\mathit{Sp} = .9063$\n",
    "* $\\mathit{Sn} = .8866$\n",
    "* $P = 329e6$\n",
    "* $p = 1000/P$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In machine learning, the chart of the TP, FP, TN, FN values\n",
    "# is called a confusion matrix. \n",
    "\n",
    "def calc_cm(p, P, Sp, Sn):\n",
    "    \n",
    "    TP = P * p * Sn\n",
    "    FP = P * (1 - p) * ( 1 - Sp )\n",
    "    TN = P * ( 1 - p ) * Sp\n",
    "    FN = P * p * ( 1- Sn )\n",
    "    \n",
    "    return ( TP, FP, TN, FN)\n",
    "\n",
    "recorded_cases = 100\n",
    "actual_cases = recorded_cases * 10\n",
    "P = 1e6 #329e6\n",
    "p = actual_cases / P\n",
    "    \n",
    "\"p={} TP={} FP={} TN={} FN={}\".format(p, *calc_cm(p, P, Sp, Sn))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this case, of 1,000 infections in the whole US, the testing would have caught 886 of them, while producing 31 million false positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def gen_data(p, Sp, Sn):\n",
    "    tests = np.linspace(1e6,250e6, num=101)\n",
    "    \n",
    "    for tn  in tests:\n",
    "        yield (tn,p)+tuple(int(e) for e in calc_cm(p, tn, Sp, Sn))\n",
    "    \n",
    "df = pd.DataFrame(list(gen_data(p,Sp, Sn)), columns='tests p TP FP TN FN'.split())\n",
    "df['p_ratio'] = (df.p*df.tests) / df.TP\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
